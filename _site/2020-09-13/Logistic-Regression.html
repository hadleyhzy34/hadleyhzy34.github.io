<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Logistic Regression | Ziyue(Hadley) Hou</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Logistic Regression" />
<meta name="author" content="Ziyue Hou" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Logistic Regression Introduction to logistic Regression This article discusses the basics of Logistic Regression and its implementation in Python. Logistic regression is basically a supervised classification algorithm. In a classification problem, the target variable(or output), y, can take only discrete values for given set of features(or inputs), X. Contrary to popular belief, logistic regression IS a regression model. The model builds a regression model to predict the probability that a given data entry belongs to the category numbered as “1”. Just like Linear regression assumes that the data follows a linear function, Logistic regression models the data using the sigmoid function." />
<meta property="og:description" content="Logistic Regression Introduction to logistic Regression This article discusses the basics of Logistic Regression and its implementation in Python. Logistic regression is basically a supervised classification algorithm. In a classification problem, the target variable(or output), y, can take only discrete values for given set of features(or inputs), X. Contrary to popular belief, logistic regression IS a regression model. The model builds a regression model to predict the probability that a given data entry belongs to the category numbered as “1”. Just like Linear regression assumes that the data follows a linear function, Logistic regression models the data using the sigmoid function." />
<link rel="canonical" href="https://hadleyhzy34.github.io/home/2020-09-13/Logistic-Regression" />
<meta property="og:url" content="https://hadleyhzy34.github.io/home/2020-09-13/Logistic-Regression" />
<meta property="og:site_name" content="Ziyue(Hadley) Hou" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-13T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Logistic Regression" />
<script type="application/ld+json">
{"headline":"Logistic Regression","url":"https://hadleyhzy34.github.io/home/2020-09-13/Logistic-Regression","datePublished":"2020-09-13T00:00:00+08:00","dateModified":"2020-09-13T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hadleyhzy34.github.io/home/2020-09-13/Logistic-Regression"},"author":{"@type":"Person","name":"Ziyue Hou"},"description":"Logistic Regression Introduction to logistic Regression This article discusses the basics of Logistic Regression and its implementation in Python. Logistic regression is basically a supervised classification algorithm. In a classification problem, the target variable(or output), y, can take only discrete values for given set of features(or inputs), X. Contrary to popular belief, logistic regression IS a regression model. The model builds a regression model to predict the probability that a given data entry belongs to the category numbered as “1”. Just like Linear regression assumes that the data follows a linear function, Logistic regression models the data using the sigmoid function.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/home/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">


  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="180x180" href="/home/assets/icon-180x180.png">
<!--   <link rel="icon" type="image/png" sizes="180x180" href="/home/assets/icon-180x180.png">
  <link rel="icon" type="image/png" sizes="150x150" href="/home/assets/icon-150x150.png">
  <link rel="icon" type="image/png" sizes="128x128" href="/home/assets/icon-128x128.png"> -->
<!--   <link rel="icon" type="image/png" sizes="70x70" href="/home/assets/icon-70x70.png">
  <link rel="icon" type="image/png" sizes="48x48" href="/home/assets/icon-48x48.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/home/assets/icon-16x16.png"> -->
  <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/home/assets/icon-180x180.png">

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="https://hadleyhzy34.github.io/home/feed.xml" title="Ziyue(Hadley) Hou" />

  <!-- Google Analytics-->
  
</head>


  <body>

    <nav class="nav">
  <div class="nav-container">
    <a href="/home/">
      <img class="site-logo" src="/home/assets/icon.png">
      <h2 class="nav-title" >Ziyue(Hadley) Hou</h2>
    </a>
    <ul>
      <li><a href="/home/">Posts</a></li>
      <li><a href="/home/category">Category</a></li>
      <li><a href="/home/archive">Archive</a></li>
      <li><a href="/home/tags">Tags</a></li>
      <li><a href="/home/about">About</a></li>
    </ul>
  </div>
</nav>


    <main>
      <div class="post">
  <div class="post-info">
    <span>Written by</span>
    
        Ziyue Hou
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2020-09-13 00:00:00 +0800">September 13, 2020</time>
    
  </div>

  <h1 class="post-title">Logistic Regression</h1>
  <div class="post-line"></div>

  <h2 id="logistic-regression">Logistic Regression</h2>
<hr />

<h3 id="introduction-to-logistic-regression">Introduction to logistic Regression</h3>
<hr />
<p>This article discusses the basics of Logistic Regression and its implementation in Python. Logistic regression is basically a supervised classification algorithm. In a classification problem, the target variable(or output), y, can take only discrete values for given set of features(or inputs), X.</p>

<p>Contrary to popular belief, logistic regression IS a regression model. The model builds a regression model to predict the probability that a given data entry belongs to the category numbered as “1”. Just like Linear regression assumes that the data follows a linear function, Logistic regression models the data using the sigmoid function.
<!--more--></p>
<h3 id="logistic-regression-model">Logistic Regression Model</h3>
<hr />
<p><img src="https://raw.githubusercontent.com/hadleyhzy34/pytorch/master/resources/sigmoid.png" alt="sigmoid" /></p>

<h3 id="logistic-regression-math">Logistic Regression Math</h3>
<hr />
<h4 id="datasets">Datasets</h4>
<hr />
<ul>
  <li>The datasets has p feature variables and n observations</li>
  <li>The feature matrix is represented as:</li>
</ul>

<p><img src="http://www.sciweavers.org/tex2img.php?eq=x%3D%20%5Cbegin%7Bbmatrix%7D1%20%26%20%20x_%7B11%7D%20%20%26%20...%20%26%20x_%7B1p%7D%5C%5C1%20%26%20%20x_%7B21%7D%20%20%26%20...%20%26%20x_%7B2p%7D%5C%5C...%20%26%20%20...%20%20%26%20...%20%26%20...%7D%5C%5C1%20%26%20%20x_%7Bn1%7D%20%20%26%20...%20%26%20x_%7Bnp%7D%20%5Cend%7Bbmatrix%7D%20&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt="x= \begin{bmatrix}1 &amp;  x_{11}  &amp; ... &amp; x_{1p}\\1 &amp;  x_{21}  &amp; ... &amp; x_{2p}\\... &amp;  ...  &amp; ... &amp; ...}\\1 &amp;  x_{n1}  &amp; ... &amp; x_{np} \end{bmatrix} " width="192" height="86" /></p>

<p>Here, <img src="https://latex.codecogs.com/svg.latex?x_{ij}" title="x_{ij}" />
denotes the values of jth feature for ith observation.
 <img src="https://latex.codecogs.com/svg.latex?x_{ij}" title="x_{ij}" />
represents the predicted response for ith observation, where ith observation can be represented as follows:
<img src="http://www.sciweavers.org/tex2img.php?eq=%20%5Coverrightarrow%7Bx_%7Bi%7D%20%7D%20%3D%5Cbegin%7Bbmatrix%7D%20x_%7Bi1%7D%20%5C%5C%20x_%7Bi2%7D%20%5C%5C%20x_%7Bi3%7D%20%5C%5C.%20%5C%5C.%20%5C%5C.%20%5C%5C%20x_%7Bip%7D%5Cend%7Bbmatrix%7D&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt=" \overrightarrow{x_{i} } =\begin{bmatrix} x_{i1} \\ x_{i2} \\ x_{i3} \\. \\. \\. \\ x_{ip}\end{bmatrix}" width="93" height="140" /> <br />
<img src="https://latex.codecogs.com/svg.latex?h(  \overrightarrow{x_{i}})" title="x_{ij}" /> represents predicted response for ith response, the formula we use for calculating <img src="https://latex.codecogs.com/svg.latex?h(  \overrightarrow{x_{i}})" title="x_{ij}" /> is called hypothesis.</p>

<h4 id="hypothesis-for-classificaiton">Hypothesis for classificaiton</h4>
<hr />
<p>In linear regression, the hypothesis we used for prediction was:
<img src="http://www.sciweavers.org/tex2img.php?eq=h%28%20%20%5Coverrightarrow%7Bx_%7Bi%7D%7D%20%20%29%20%3D%20%20%20%5Cbeta%20_%7B0%7D%20%2B%20%5Cbeta%20_%7B1%7D%20x_%7Bi1%7D%20%2B%20%5Cbeta%20_%7B2%7D%20x_%7Bi2%7D%2B...%2B%20%5Cbeta%20_%7Bp%7D%20x_%7Bip%7D&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt="h(  \overrightarrow{x_{i}}  ) =   \beta _{0} + \beta _{1} x_{i1} + \beta _{2} x_{i2}+...+ \beta _{p} x_{ip}" width="317" height="28" /></p>

<h4 id="modification-on-linear-regression">Modification on Linear Regression</h4>
<p>some modifications are made to the hypothesis for classification:<br />
<img src="http://www.sciweavers.org/tex2img.php?eq=h%28%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%29%3D%20%5Cfrac%7B1%7D%7B1%2B%20e%5E%7B-%20%20%5Cbeta%20%5E%7BT%7D%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%7D%7D&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt="h( \overrightarrow{ x_{i} })= \frac{1}{1+ e^{-  \beta ^{T} \overrightarrow{ x_{i} }}}" width="154" height="46" /></p>

<p>where regression coefficient vector be:
<img src="http://www.sciweavers.org/tex2img.php?eq=%5Coverrightarrow%7B%5Cbeta%7D%20%20%20%3D%20%5Cbegin%7Bbmatrix%7D%20%20%5Cbeta%20_%7B0%7D%20%20%5C%5C%20%5Cbeta%20_%7B1%7D%5C%5C%20%5Cbeta%20_%7B2%7D%5C%5C%20.%5C%5C%20.%5C%5C%20.%5C%5C%20%5Cbeta%20_%7Bp%7D%5Cend%7Bbmatrix%7D&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt="\overrightarrow{\beta}   = \begin{bmatrix}  \beta _{0}  \\ \beta _{1}\\ \beta _{2}\\ .\\ .\\ .\\ \beta _{p}\end{bmatrix}" width="89" height="140" /></p>

<p>Note that 
 <img src="https://latex.codecogs.com/svg.latex?\beta _{0}" title="x_{ij}" />
 represents bias of linear regression hypothesis.</p>

<h4 id="conditional-probabilities-based-ith-observations">Conditional Probabilities based ith observations:</h4>
<hr />
<p><img src="http://www.sciweavers.org/tex2img.php?eq=%20p%28y_%7Bi%7D%3D1%20%7C%20%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%2C%20%20%5Coverrightarrow%7B%20%5Cbeta%20%7D%20%29%20%20%3D%20h%28%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%20%29&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt=" p(y_{i}=1 |  \overrightarrow{ x_{i} },  \overrightarrow{ \beta } )  = h( \overrightarrow{ x_{i} } )" width="194" height="29" /></p>

<p><img src="http://www.sciweavers.org/tex2img.php?eq=%20p%28y_%7Bi%7D%3D0%20%7C%20%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%2C%20%20%5Coverrightarrow%7B%20%5Cbeta%20%7D%20%29%20%20%3D%201-h%28%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%20%29&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt=" p(y_{i}=0 |  \overrightarrow{ x_{i} },  \overrightarrow{ \beta } )  = 1-h( \overrightarrow{ x_{i} } )" width="225" height="29" /></p>

<h4 id="compact-form">Compact form</h4>
<hr />
<p><img src="http://www.sciweavers.org/tex2img.php?eq=p%28y_%7Bi%7D%20%7C%20%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%2C%20%20%5Coverrightarrow%7B%20%5Cbeta%20%7D%20%29%20%20%3D%20h%28%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%20%29%20%5E%7B%20y_%7Bi%7D%20%7D%20%281-h%28%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%20%29%29%20%5E%7B1-%20y_%7Bi%7D%20%7D&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt="p(y_{i} |  \overrightarrow{ x_{i} },  \overrightarrow{ \beta } )  = h( \overrightarrow{ x_{i} } ) ^{ y_{i} } (1-h( \overrightarrow{ x_{i} } )) ^{1- y_{i} }" width="294" height="29" /></p>

<p>Note that yi represents the ith observation categorical target. Image ith observation categorical target is 1, then we need to maximize ith prediciton reponse as large as 1. While if ith observation categorical target is 0, we need to maximize <img src="https://latex.codecogs.com/svg.latex?1-h( \overrightarrow{ x_{i} } )" title="x_{ij}" />. In general, maximize <img src="https://latex.codecogs.com/svg.latex?p(y_{i} |  \overrightarrow{ x_{i} },  \overrightarrow{ \beta } )" title="x_{ij}" /> will fit more our model and obtain better result. Since x and y are observation values, we estimat the parameter <img src="https://latex.codecogs.com/svg.latex?\overrightarrow{ \beta } " title="x_{ij}" /> to maximizing this likelihood function:
<img src="http://www.sciweavers.org/tex2img.php?eq=L%28%20%5Coverrightarrow%7B%20%5Cbeta%20%7D%20%29%20%3D%20%5Cprod_i%5En%20h%28%20%5Coverrightarrow%7Bx_%7Bi%7D%7D%20%29%20%5E%7B%20y_%7Bi%7D%20%7D%20%281-h%28%20%5Coverrightarrow%7Bx_%7Bi%7D%7D%20%29%20%29%5E%7B%281-y_%7Bi%7D%29%7D&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt="L( \overrightarrow{ \beta } ) = \prod_i^n h( \overrightarrow{x_{i}} ) ^{ y_{i} } (1-h( \overrightarrow{x_{i}} ) )^{(1-y_{i})}" width="281" height="50" /></p>

<p>Take log likelihood and obtain cost function:
<img src="http://www.sciweavers.org/tex2img.php?eq=J%28%20%5Coverrightarrow%7B%20%5Cbeta%20%7D%20%29%20%3D%20%20%5Csum_i%5En%20-%20y_%7Bi%7D%20h%28%20%5Coverrightarrow%7Bx_%7Bi%7D%7D%20%29%20-%20%281-y_%7Bi%7D%29%281-h%28%20%5Coverrightarrow%7Bx_%7Bi%7D%7D%20%29%29&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt="J( \overrightarrow{ \beta } ) =  \sum_i^n - y_{i} h( \overrightarrow{x_{i}} ) - (1-y_{i})(1-h( \overrightarrow{x_{i}} ))" width="332" height="50" /></p>

<h4 id="using-gradient-descent-algorithm">Using Gradient descent algorithm</h4>
<hr />
<p><img src="http://www.sciweavers.org/tex2img.php?eq=%20%5Cfrac%7B%5Cpartial%20J%28%20%5Coverrightarrow%7B%20%5Cbeta%20%7D%20%29%7D%7B%5Cpartial%20%20%20%5Cbeta%20_%7Bj%7D%20%7D%20%3D%20%20%5Csum_i%5En%20%20x_%7Bij%7D%20%20h%28%20%5Coverrightarrow%7Bx_%7Bi%7D%7D%20%29%20-%20x_%7Bij%7D%20%20%5Cwidehat%7B%20y_%7Bi%7D%20%7D%20%3D%20%28h%28%20%5Coverrightarrow%7Bx%7D%20%29%20-%20%5Cwidehat%7B%20y%20%7D%29%20%5Cbullet%20%20%5Coverrightarrow%7B%20x_%7Bi%7D%20%7D%20&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0" align="center" border="0" alt=" \frac{\partial J( \overrightarrow{ \beta } )}{\partial   \beta _{j} } =  \sum_i^n  x_{ij}  h( \overrightarrow{x_{i}} ) - x_{ij}  \widehat{ y_{i} } = (h( \overrightarrow{x} ) - \widehat{ y }) \bullet  \overrightarrow{ x_{i} } " width="362" height="60" /></p>

<h2 id="logistic-regression-using-python-from-scratch">Logistic Regression using python from scratch</h2>
<hr />
<h3 id="import-modules">import modules</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</code></pre></div></div>

<h3 id="prepare-data">Prepare Data</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'/Users/hadley/Documents/pytorch/resources/heart.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(303, 14)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#### Creating Dummy Variables for categorical variables
</span><span class="n">dummy1</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'cp'</span><span class="p">],</span> <span class="n">prefix</span> <span class="o">=</span> <span class="s">"cp"</span><span class="p">)</span>
<span class="n">dummy2</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'thal'</span><span class="p">],</span> <span class="n">prefix</span> <span class="o">=</span> <span class="s">"thal"</span><span class="p">)</span>
<span class="n">dummy3</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'slope'</span><span class="p">],</span> <span class="n">prefix</span> <span class="o">=</span> <span class="s">"slope"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">dummy1</span><span class="p">,</span> <span class="n">dummy2</span><span class="p">,</span> <span class="n">dummy3</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">dummy1</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dummy2</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dummy3</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(303, 4) (303, 4) (303, 3)
</code></pre></div></div>

<p>currently data is still list object, not ndarray</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>cp</th>
      <th>trestbps</th>
      <th>chol</th>
      <th>fbs</th>
      <th>restecg</th>
      <th>thalach</th>
      <th>exang</th>
      <th>oldpeak</th>
      <th>...</th>
      <th>cp_1</th>
      <th>cp_2</th>
      <th>cp_3</th>
      <th>thal_0</th>
      <th>thal_1</th>
      <th>thal_2</th>
      <th>thal_3</th>
      <th>slope_0</th>
      <th>slope_1</th>
      <th>slope_2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>63</td>
      <td>1</td>
      <td>3</td>
      <td>145</td>
      <td>233</td>
      <td>1</td>
      <td>0</td>
      <td>150</td>
      <td>0</td>
      <td>2.3</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>37</td>
      <td>1</td>
      <td>2</td>
      <td>130</td>
      <td>250</td>
      <td>0</td>
      <td>1</td>
      <td>187</td>
      <td>0</td>
      <td>3.5</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>41</td>
      <td>0</td>
      <td>1</td>
      <td>130</td>
      <td>204</td>
      <td>0</td>
      <td>0</td>
      <td>172</td>
      <td>0</td>
      <td>1.4</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>56</td>
      <td>1</td>
      <td>1</td>
      <td>120</td>
      <td>236</td>
      <td>0</td>
      <td>1</td>
      <td>178</td>
      <td>0</td>
      <td>0.8</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>57</td>
      <td>0</td>
      <td>0</td>
      <td>120</td>
      <td>354</td>
      <td>0</td>
      <td>1</td>
      <td>163</td>
      <td>1</td>
      <td>0.6</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 25 columns</p>
</div>

<p>drop previous categorical columns:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'cp'</span><span class="p">,</span> <span class="s">'thal'</span><span class="p">,</span> <span class="s">'slope'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>trestbps</th>
      <th>chol</th>
      <th>fbs</th>
      <th>restecg</th>
      <th>thalach</th>
      <th>exang</th>
      <th>oldpeak</th>
      <th>ca</th>
      <th>...</th>
      <th>cp_1</th>
      <th>cp_2</th>
      <th>cp_3</th>
      <th>thal_0</th>
      <th>thal_1</th>
      <th>thal_2</th>
      <th>thal_3</th>
      <th>slope_0</th>
      <th>slope_1</th>
      <th>slope_2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>63</td>
      <td>1</td>
      <td>145</td>
      <td>233</td>
      <td>1</td>
      <td>0</td>
      <td>150</td>
      <td>0</td>
      <td>2.3</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>37</td>
      <td>1</td>
      <td>130</td>
      <td>250</td>
      <td>0</td>
      <td>1</td>
      <td>187</td>
      <td>0</td>
      <td>3.5</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>41</td>
      <td>0</td>
      <td>130</td>
      <td>204</td>
      <td>0</td>
      <td>0</td>
      <td>172</td>
      <td>0</td>
      <td>1.4</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>56</td>
      <td>1</td>
      <td>120</td>
      <td>236</td>
      <td>0</td>
      <td>1</td>
      <td>178</td>
      <td>0</td>
      <td>0.8</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>57</td>
      <td>0</td>
      <td>120</td>
      <td>354</td>
      <td>0</td>
      <td>1</td>
      <td>163</td>
      <td>1</td>
      <td>0.6</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 22 columns</p>
</div>

<h3 id="creating-model-for-logistic-regression">Creating Model for logistic regression</h3>
<hr />
<p>split data into input observations and output observations</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(303,)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'target'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(303, 21)
</code></pre></div></div>

<h3 id="normalize-data">Normalize Data</h3>
<hr />
<p>normalize input observation data and add one more ‘dummy’ feature column so that bias calculation included here:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">)).</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">bias_column</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">bias_column</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(303, 22)
</code></pre></div></div>

<p>Split data, 80% will be train data and 20% will be test data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(242, 22) (61, 22) (242,) (61,)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### Weight and Bias Initialization, note that n_features+1 represent weights w already included bias
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_features</span><span class="p">,))</span>
<span class="k">print</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>242 22 (22,)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#model prediction: forward pass by using sigmoid function
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</code></pre></div></div>

<p>loss function based on maximum likelihood estimation</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">y_train</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y_train</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y_predicted</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">/</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="gradient-of-weight">gradient of weight</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_weight</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">):</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">).</span><span class="n">T</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dw</span>

<span class="c1">## training 
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">100</span>
</code></pre></div></div>

<h3 id="forward-predict">forward predict</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_predict</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="n">y_prediction</span> <span class="o">=</span>  <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_prediction</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">y_prediction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">y_prediction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_prediction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">y_prediction</span>
</code></pre></div></div>

<h3 id="prepare-for-loss-curve-number-of-iteration-as-x-axis-and-loss-as-y-axis">Prepare for loss curve, number of iteration as x axis and loss as y axis</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt_x</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">plt_y</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<h3 id="training">Training</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
    <span class="c1">#prediction = forward pass
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

    <span class="c1">#loss
</span>    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="c1">#gradients
</span>    <span class="n">dw</span> <span class="o">=</span> <span class="n">gradient_weight</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="c1">#update wrights
</span>    <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dw</span>

    <span class="n">plt_x</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="n">plt_y</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">1</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: loss = </span><span class="si">{</span><span class="n">l</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch 1: loss = 0.5536
epoch 2: loss = 0.4194
epoch 3: loss = 0.3742
epoch 4: loss = 0.3535
epoch 5: loss = 0.3418
epoch 6: loss = 0.3343
epoch 7: loss = 0.3290
epoch 8: loss = 0.3250
epoch 9: loss = 0.3218
epoch 10: loss = 0.3191
epoch 11: loss = 0.3167
epoch 12: loss = 0.3147
epoch 13: loss = 0.3128
epoch 14: loss = 0.3111
epoch 15: loss = 0.3095
epoch 16: loss = 0.3080
epoch 17: loss = 0.3066
epoch 18: loss = 0.3053
epoch 19: loss = 0.3041
epoch 20: loss = 0.3029
epoch 21: loss = 0.3018
epoch 22: loss = 0.3008
epoch 23: loss = 0.2998
epoch 24: loss = 0.2989
epoch 25: loss = 0.2979
epoch 26: loss = 0.2971
epoch 27: loss = 0.2963
epoch 28: loss = 0.2955
epoch 29: loss = 0.2947
epoch 30: loss = 0.2940
epoch 31: loss = 0.2933
epoch 32: loss = 0.2926
epoch 33: loss = 0.2920
epoch 34: loss = 0.2913
epoch 35: loss = 0.2907
epoch 36: loss = 0.2902
epoch 37: loss = 0.2896
epoch 38: loss = 0.2891
epoch 39: loss = 0.2886
epoch 40: loss = 0.2881
epoch 41: loss = 0.2876
epoch 42: loss = 0.2871
epoch 43: loss = 0.2867
epoch 44: loss = 0.2862
epoch 45: loss = 0.2858
epoch 46: loss = 0.2854
epoch 47: loss = 0.2850
epoch 48: loss = 0.2846
epoch 49: loss = 0.2843
epoch 50: loss = 0.2839
epoch 51: loss = 0.2836
epoch 52: loss = 0.2832
epoch 53: loss = 0.2829
epoch 54: loss = 0.2826
epoch 55: loss = 0.2823
epoch 56: loss = 0.2820
epoch 57: loss = 0.2817
epoch 58: loss = 0.2814
epoch 59: loss = 0.2812
epoch 60: loss = 0.2809
epoch 61: loss = 0.2806
epoch 62: loss = 0.2804
epoch 63: loss = 0.2801
epoch 64: loss = 0.2799
epoch 65: loss = 0.2797
epoch 66: loss = 0.2795
epoch 67: loss = 0.2792
epoch 68: loss = 0.2790
epoch 69: loss = 0.2788
epoch 70: loss = 0.2786
epoch 71: loss = 0.2784
epoch 72: loss = 0.2782
epoch 73: loss = 0.2780
epoch 74: loss = 0.2779
epoch 75: loss = 0.2777
epoch 76: loss = 0.2775
epoch 77: loss = 0.2773
epoch 78: loss = 0.2772
epoch 79: loss = 0.2770
epoch 80: loss = 0.2769
epoch 81: loss = 0.2767
epoch 82: loss = 0.2766
epoch 83: loss = 0.2764
epoch 84: loss = 0.2763
epoch 85: loss = 0.2761
epoch 86: loss = 0.2760
epoch 87: loss = 0.2759
epoch 88: loss = 0.2757
epoch 89: loss = 0.2756
epoch 90: loss = 0.2755
epoch 91: loss = 0.2753
epoch 92: loss = 0.2752
epoch 93: loss = 0.2751
epoch 94: loss = 0.2750
epoch 95: loss = 0.2749
epoch 96: loss = 0.2748
epoch 97: loss = 0.2747
epoch 98: loss = 0.2746
epoch 99: loss = 0.2745
epoch 100: loss = 0.2744
</code></pre></div></div>

<p>loss curve, loss function returned value with respect to number of iterations</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plt_x</span><span class="p">,</span> <span class="n">plt_y</span><span class="p">,</span> <span class="s">'b'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"number of epochs"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"MLE cost function"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.figsize"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/hadleyhzy34/pytorch/master/resources/loss_curve_logistic_regression.png" alt="svg" /></p>

<h3 id="manually-calculate-accuracy">manually calculate accuracy</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">forward_predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Manuel Test Accuracy: {:.2f}%"</span><span class="p">.</span><span class="nb">format</span><span class="p">((</span><span class="mi">100</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">y_test_pred</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Manuel Test Accuracy: 85.25%
</code></pre></div></div>

<p>Final plot to check distance between predicted value and observation with repect to each feature</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_linear</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
     <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_linear</span><span class="p">)</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">t1</span><span class="p">),</span> <span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_linear</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"number of epochs"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"MLE cost function"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.figsize"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[-2.29420457 -0.06308867  0.04797952 -3.46814858 -2.0499066  -1.12642636
 -2.5685351  -2.41063596 -3.50555731 -4.49125426  0.60393739  2.5569347
 -3.00098277  2.30837334  3.2512419   1.25612815 -2.71948701  1.44747319
 -4.28088693  1.39343452  1.46533894 -1.24259217 -1.81542927 -1.63853973
  2.4237399   0.25793775 -1.76087883 -0.68070708  3.49235634  1.37801868
  1.25534473 -4.33003805  3.67578518  0.98659795  2.67926752  0.57409612
 -2.1883085   2.22040851 -1.85645047 -0.36110886 -0.50539455  1.48557235
  0.04146691 -1.75997446  0.44119235  1.63330185  1.78237282 -0.07312052
 -2.78411445  1.1278409   2.39580181  1.31927031  3.61757349  0.59014531
  4.81296128 -2.13979847  2.71081302  2.07815489  2.19097508  3.4580257
  1.06761226]
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/hadleyhzy34/pytorch/master/resources/sigmoid_result.png" alt="svg" /></p>

</div>



<div class="pagination">
  
    <a href="/home/2020-09-17/kth-nearest-neighbor" class="left arrow">&#8592;</a>
  
  
    <a href="/home/2020-09-11/Linear-Regression" class="right arrow">&#8594;</a>
  

  <a href="#" class="top">Top</a>
</div>
    </main>

    <footer class= "blog-footer">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
          <p>
            <a href="#" class="fa fa-facebook"></a>
            <a href="#" class="fa fa-twitter"></a>
            <a href="#" class="fa fa-github"></a>
            <a href="#" class="fa fa-linkedin"></a>
            <a href="#" class="fa fa-wechat"></a>
            <a href="#" class="fa fa-weibo"></a>
            <a href="#" class="fa fa-google"></a>
            <a href="#" class="fa fa-skype"></a>
          </p>
        
<!--     <p>Hadley_hzy@hotmail.com</p> -->

    <p>© Ziyue(Hadley) Hou 2021<!--  --></p>

  </footer>

  </body>
</html>
